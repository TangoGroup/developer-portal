---
title: "Our AI Models"
description: "AI + Human > AI - Human"
---

# What Makes Gloo AIâ€™s Models Different ðŸ¤¨

At Gloo AI, we donâ€™t build general-purpose LLMs. We build mission-specific models engineered to serve the real needs of people, communities, and organizations â€” with a focus on **Human Flourishing**. Our approach diverges from traditional foundation model labs in several critical ways:

## 1. Faith-Aligned + Values-Aware Modeling

Glooâ€™s models are trained and fine-tuned with data and evaluation frameworks that reflect the values, questions, and complexity of faith-driven and human-centered domains. Our benchmarks donâ€™t just score factual accuracy or reasoning â€” they evaluate alignment to **7 dimensions of human flourishing**, including meaning, purpose, and relational health.

* \~3,000+ handcrafted and sourced questions
* Dual evaluation: objective correctness + value alignment
* Faith-specific QA from real ministry and worldview contexts

Our suite of fine-tuned models is defined to give you the most accurate, faith and human flourishing-informed answers - whether you're exploring moral scenarios, seeking guidance on living out biblical principles, or diving into broader human-flourishing topics.

We've launched an experimental endpoint that returns more biblically aligned responses and practical, scripture-rooted guidance. It's an early prototype so we welcome feedback to find ways to grow and better serve your needs. To try it out, check out this page on how to get started:

## 2. Publisher + Org-Aware Context Handling

Our models respect multi-tenant structure at the architectural level. Every retrieval-augmented response is scoped to the **Org and Publisher context**, ensuring:

* No cross-org data contamination
* Retrieval pipelines tuned per-publisher (e.g., Church Aâ€™s content is separate from Publisher B)
* Responses are shaped by **localized embeddings**, not a monolithic corpus

This makes our approach **safe, scoped, and contextual** â€” critical for trusted usage in faith-based and community environments.

## 3. Transparent and Auditable Evaluation Tools

We donâ€™t just evaluate models â€” we **show our work**. Glooâ€™s evaluation engine:

* Runs reproducible tests across top open-source and commercial models (Gemini, DeepSeek, Grok, Mistral, etc.)
* Scores models across **subjective, objective, and tangential** response types
* Tests modelsâ€™ ability to **evaluate others**, not just generate responses

This allows us to detect hallucinations, failure patterns, and alignment drift in ways most labs donâ€™t expose â€” and lets our partners understand **how and why** models behave the way they do.

## 4. Composable, Multimodal Training Loops

We donâ€™t treat fine-tuning as a one-shot process. Glooâ€™s model workflows are built on:

* Composable training pipelines: swap datasets, prompts, and evaluation metrics at will
* Support for multi-format inputs (PDF, audio, text, metadata-rich content)
* Fine-tuning + retrieval pipelines that **co-evolve** based on actual use

This lets us continually refine performance in production-like settings â€” especially for Assistant and Studio use cases where grounding, tone, and logic matter deeply.

## 5. Safe Defaults + Guardrails for Trust

Every Gloo AI system ships with embedded safety:

* No unfiltered general web data in training
* Explicit rejection training on unsafe or misleading completions
* Org-level filters and Publisher-specific prompts prevent misuse at scale
* All outputs are tied to **retrievable sources**, where applicable

> Trust isnâ€™t just a feature. Itâ€™s a **requirement** for the communities we serve.

***

Whatâ€™s Next

* [Quickstart for Developers](/docs/quickstart-for-developers)

* [Table of Contents](#)

* * [What Makes Gloo AIâ€™s Models Different ðŸ¤¨](#what-makes-gloo-ais-models-different-)

  * * [1. Faith-Aligned + Values-Aware Modeling](#1-faith-aligned--values-aware-modeling)
    * [2. Publisher + Org-Aware Context Handling](#2-publisher--org-aware-context-handling)
    * [3. Transparent and Auditable Evaluation Tools](#3-transparent-and-auditable-evaluation-tools)
    * [4. Composable, Multimodal Training Loops](#4-composable-multimodal-training-loops)
    * [5. Safe Defaults + Guardrails for Trust](#5-safe-defaults--guardrails-for-trust)
